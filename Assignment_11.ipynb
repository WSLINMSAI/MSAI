{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNBiKhSYgLApHLifdk4VD77",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WSLINMSAI/MSAI-531-B01/blob/main/Assignment_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab Ready Alice Text Generator - Character Level\n",
        "# - Non stateful GRU for training with shuffled sequences\n",
        "# - Separate stateful sampler for generation\n",
        "# - Safe ASCII charset cleaning to avoid odd symbols\n",
        "# - Last timestep logits with temperature sampling\n",
        "# - Keras 3 compatible checkpoints ending in .weights.h5\n",
        "# - NEW: sample text every N epochs with a fixed seed and length\n",
        "\n",
        "import os, io, string, numpy as np, tensorflow as tf\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "try:\n",
        "    devs = tf.config.list_physical_devices(\"GPU\")\n",
        "    print(\"GPU available:\", bool(devs), devs)\n",
        "except Exception as e:\n",
        "    print(\"GPU check error:\", e)\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "EPOCHS = 50\n",
        "SEQ_LENGTH = 100\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "EMBED_DIM = 256\n",
        "RNN_UNITS = 512\n",
        "LEARNING_RATE = 1e-3\n",
        "CLIPNORM = 1.0\n",
        "\n",
        "TEMPERATURE = 0.8\n",
        "GEN_LEN = 1000              # final sample length after training\n",
        "SEED_TEXT = \"Alice \"\n",
        "\n",
        "SAMPLE_EVERY = 10           # generate during training every N epochs\n",
        "SAMPLE_LEN = 400            # sample length for the periodic samples\n",
        "SAMPLE_SEED = \"Alice \"      # standard prefix used each time\n",
        "\n",
        "CHECKPOINT_DIR = \"./checkpoints_alice_clean\"\n",
        "LOCAL_TEXT_PATH = \"/content/alice_in_wonderland.txt\"  # upload a file here to override\n",
        "\n",
        "# -----------------------------\n",
        "# Text loading and cleaning\n",
        "# -----------------------------\n",
        "def load_text():\n",
        "    # Use local file if present, else download from Gutenberg\n",
        "    if os.path.exists(LOCAL_TEXT_PATH):\n",
        "        with io.open(LOCAL_TEXT_PATH, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            return f.read()\n",
        "    url = \"https://www.gutenberg.org/files/11/11-0.txt\"\n",
        "    path = tf.keras.utils.get_file(\"alice_in_wonderland.txt\", origin=url)\n",
        "    with io.open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def clean_to_ascii(text):\n",
        "    # Keep ASCII letters, digits, punctuation, space, newline, tab\n",
        "    allowed = set(string.ascii_letters + string.digits + string.punctuation + \" \\n\\t\")\n",
        "    cleaned = \"\".join(c if c in allowed else \" \" for c in text)\n",
        "    # Collapse whitespace to avoid long runs\n",
        "    cleaned = \" \".join(cleaned.split())\n",
        "    # Add simple newlines after periods for readability\n",
        "    cleaned = cleaned.replace(\". \", \".\\n\")\n",
        "    return cleaned\n",
        "\n",
        "# -----------------------------\n",
        "# Vectorization\n",
        "# -----------------------------\n",
        "def build_vocab(text):\n",
        "    vocab = sorted(list(set(text)))\n",
        "    char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    return vocab, char2idx, idx2char\n",
        "\n",
        "def text_to_ids(text, char2idx):\n",
        "    return np.array([char2idx[c] for c in text], dtype=np.int32)\n",
        "\n",
        "def make_dataset(ids, seq_length, batch_size, buffer_size):\n",
        "    char_ds = tf.data.Dataset.from_tensor_slices(ids)\n",
        "    sequences = char_ds.batch(seq_length + 1, drop_remainder=True)\n",
        "\n",
        "    def split_input_target(chunk):\n",
        "        return chunk[:-1], chunk[1:]\n",
        "\n",
        "    ds = sequences.map(split_input_target, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    ds = ds.shuffle(buffer_size).batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "# -----------------------------\n",
        "# Model\n",
        "# -----------------------------\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size, stateful):\n",
        "    if stateful:\n",
        "        inputs = tf.keras.Input(batch_shape=[batch_size, None], dtype=tf.int32)\n",
        "    else:\n",
        "        inputs = tf.keras.Input(shape=(None,), dtype=tf.int32)\n",
        "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "    x = tf.keras.layers.GRU(\n",
        "        rnn_units,\n",
        "        return_sequences=True,\n",
        "        stateful=stateful,\n",
        "        recurrent_initializer=\"glorot_uniform\",\n",
        "        name=\"rnn\"  # name the GRU so we can reset its state later\n",
        "    )(x)\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "    return tf.keras.Model(inputs, outputs, name=\"CharRNN\")\n",
        "\n",
        "# -----------------------------\n",
        "# Training\n",
        "# -----------------------------\n",
        "def loss_fn(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "def train_model(train_ds, vocab_size, extra_callbacks=None):\n",
        "    model = build_model(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=EMBED_DIM,\n",
        "        rnn_units=RNN_UNITS,\n",
        "        batch_size=None,   # flexible batch for non stateful training\n",
        "        stateful=False\n",
        "    )\n",
        "    opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, clipnorm=CLIPNORM)\n",
        "    model.compile(optimizer=opt, loss=loss_fn)\n",
        "\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    ckpt_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(CHECKPOINT_DIR, \"epoch{epoch:02d}.weights.h5\"),\n",
        "        save_weights_only=True,\n",
        "        save_best_only=False,\n",
        "        verbose=1,\n",
        "    )\n",
        "    es_cb = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, restore_best_weights=True)\n",
        "\n",
        "    callbacks = [ckpt_cb, es_cb]\n",
        "    if extra_callbacks:\n",
        "        callbacks += list(extra_callbacks)\n",
        "\n",
        "    print(\"Training...\")\n",
        "    history = model.fit(train_ds, epochs=EPOCHS, callbacks=callbacks)\n",
        "    return model, history\n",
        "\n",
        "# -----------------------------\n",
        "# Sampling helpers\n",
        "# -----------------------------\n",
        "@tf.function\n",
        "def sample_step(model, input_id, temperature=1.0):\n",
        "    logits = model(input_id)            # shape [1, 1, vocab]\n",
        "    logits = logits[:, -1, :] / temperature\n",
        "    next_id = tf.random.categorical(logits, num_samples=1)[0, 0]\n",
        "    return next_id\n",
        "\n",
        "def build_stateful_sampler(vocab_size, trained_model):\n",
        "    sampler = build_model(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=EMBED_DIM,\n",
        "        rnn_units=RNN_UNITS,\n",
        "        batch_size=1,\n",
        "        stateful=True\n",
        "    )\n",
        "    sampler.set_weights(trained_model.get_weights())\n",
        "    return sampler\n",
        "\n",
        "def reset_rnn_states(model):\n",
        "    # Reset states of any stateful RNN layers\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, \"reset_states\"):\n",
        "            layer.reset_states()\n",
        "\n",
        "def generate_text(sampler, start_string, char2idx, idx2char, gen_len, temperature):\n",
        "    # Convert seed to ids and prime the state with the whole seed\n",
        "    input_ids = tf.expand_dims([char2idx[c] for c in start_string], 0)  # [1, T]\n",
        "    reset_rnn_states(sampler)\n",
        "    _ = sampler(input_ids)                    # prime with seed\n",
        "    next_input = input_ids[:, -1:]            # start from last seed char\n",
        "\n",
        "    out_chars = []\n",
        "    for _ in range(gen_len):\n",
        "        nid = sample_step(sampler, next_input, temperature=temperature)\n",
        "        nid_val = nid.numpy()\n",
        "        out_chars.append(idx2char[nid_val])\n",
        "        next_input = tf.expand_dims([nid_val], 0)\n",
        "    return start_string + \"\".join(out_chars)\n",
        "\n",
        "# -----------------------------\n",
        "# Callback to sample during training every N epochs\n",
        "# -----------------------------\n",
        "class TextSamplerCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, char2idx, idx2char, seed_text, gen_len, temperature, vocab_size, every=10):\n",
        "        super().__init__()\n",
        "        self.char2idx = char2idx\n",
        "        self.idx2char = idx2char\n",
        "        self.seed_text = seed_text\n",
        "        self.gen_len = gen_len\n",
        "        self.temperature = temperature\n",
        "        self.vocab_size = vocab_size\n",
        "        self.every = every\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.every == 0:\n",
        "            sampler = build_stateful_sampler(self.vocab_size, self.model)\n",
        "            sample = generate_text(\n",
        "                sampler,\n",
        "                start_string=self.seed_text,\n",
        "                char2idx=self.char2idx,\n",
        "                idx2char=self.idx2char,\n",
        "                gen_len=self.gen_len,\n",
        "                temperature=self.temperature\n",
        "            )\n",
        "            print(\"\\n\" + \"=\" * 80)\n",
        "            print(f\"Sample after epoch {epoch + 1}:\")\n",
        "            print(sample)\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "# -----------------------------\n",
        "# Main\n",
        "# -----------------------------\n",
        "raw_text = load_text()\n",
        "print(\"Raw length:\", len(raw_text))\n",
        "\n",
        "text = clean_to_ascii(raw_text)\n",
        "print(\"Clean length:\", len(text))\n",
        "print(\"\\nSample of cleaned text:\\n\", text[:400])\n",
        "\n",
        "vocab, char2idx, idx2char = build_vocab(text)\n",
        "print(\"Vocab size:\", len(vocab))\n",
        "\n",
        "ids = text_to_ids(text, char2idx)\n",
        "ds = make_dataset(ids, SEQ_LENGTH, BATCH_SIZE, BUFFER_SIZE)\n",
        "\n",
        "sampler_cb = TextSamplerCallback(\n",
        "    char2idx=char2idx,\n",
        "    idx2char=idx2char,\n",
        "    seed_text=SAMPLE_SEED,\n",
        "    gen_len=SAMPLE_LEN,\n",
        "    temperature=TEMPERATURE,\n",
        "    vocab_size=len(vocab),\n",
        "    every=SAMPLE_EVERY\n",
        ")\n",
        "\n",
        "model, history = train_model(ds, vocab_size=len(vocab), extra_callbacks=[sampler_cb])\n",
        "\n",
        "sampler = build_stateful_sampler(len(vocab), model)\n",
        "final_sample = generate_text(\n",
        "    sampler,\n",
        "    start_string=SEED_TEXT,\n",
        "    char2idx=char2idx,\n",
        "    idx2char=idx2char,\n",
        "    gen_len=GEN_LEN,\n",
        "    temperature=TEMPERATURE\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Final sample after training:\")\n",
        "print(final_sample)\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiKP5P_EidVc",
        "outputId": "1a64bddf-b8cb-454f-ddb4-3323be69621f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n",
            "GPU available: True [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Raw length: 144696\n",
            "Clean length: 140805\n",
            "\n",
            "Sample of cleaned text:\n",
            " *** START OF THE PROJECT GUTENBERG EBOOK 11 *** [Illustration] Alice s Adventures in Wonderland by Lewis Carroll THE MILLENNIUM FULCRUM EDITION 3.0 Contents CHAPTER I.\n",
            "Down the Rabbit-Hole CHAPTER II.\n",
            "The Pool of Tears CHAPTER III.\n",
            "A Caucus-Race and a Long Tale CHAPTER IV.\n",
            "The Rabbit Sends in a Little Bill CHAPTER V.\n",
            "Advice from a Caterpillar CHAPTER VI.\n",
            "Pig and Pepper CHAPTER VII.\n",
            "A Mad Tea-Party\n",
            "Vocab size: 70\n",
            "Training...\n",
            "Epoch 1/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 3.7022\n",
            "Epoch 1: saving model to ./checkpoints_alice_clean/epoch01.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - loss: 3.6829\n",
            "Epoch 2/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.5884\n",
            "Epoch 2: saving model to ./checkpoints_alice_clean/epoch02.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 2.5765\n",
            "Epoch 3/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.3124\n",
            "Epoch 3: saving model to ./checkpoints_alice_clean/epoch03.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 2.3075\n",
            "Epoch 4/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.1790\n",
            "Epoch 4: saving model to ./checkpoints_alice_clean/epoch04.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 2.1757\n",
            "Epoch 5/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.0799\n",
            "Epoch 5: saving model to ./checkpoints_alice_clean/epoch05.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 2.0763\n",
            "Epoch 6/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.9732\n",
            "Epoch 6: saving model to ./checkpoints_alice_clean/epoch06.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 1.9724\n",
            "Epoch 7/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.8900\n",
            "Epoch 7: saving model to ./checkpoints_alice_clean/epoch07.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.8889\n",
            "Epoch 8/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.7934\n",
            "Epoch 8: saving model to ./checkpoints_alice_clean/epoch08.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.7925\n",
            "Epoch 9/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.7290\n",
            "Epoch 9: saving model to ./checkpoints_alice_clean/epoch09.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.7265\n",
            "Epoch 10/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.6587\n",
            "Epoch 10: saving model to ./checkpoints_alice_clean/epoch10.weights.h5\n",
            "\n",
            "================================================================================\n",
            "Sample after epoch 10:\n",
            "Alice atain the war gongly.\n",
            "at the Whis had fack turn t any spomped in had oup of parsiom, and reverstily: and they meling the ouper.\n",
            "Of, the she said the Mouse of a timy good the mommouse, thought the offyAlly did ouf away whoPnes cromem than herpelf; so here wound _ mush the looked of urunting to the Dirst, _ut west of tith stopled.\n",
            "Hece said they wikn the hadd of the reches.\n",
            "It you, shat the Gryphen \n",
            "================================================================================\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 1.6581\n",
            "Epoch 11/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.6058\n",
            "Epoch 11: saving model to ./checkpoints_alice_clean/epoch11.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - loss: 1.6047\n",
            "Epoch 12/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.5254\n",
            "Epoch 12: saving model to ./checkpoints_alice_clean/epoch12.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.5248\n",
            "Epoch 13/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.4697\n",
            "Epoch 13: saving model to ./checkpoints_alice_clean/epoch13.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 1.4696\n",
            "Epoch 14/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.4311\n",
            "Epoch 14: saving model to ./checkpoints_alice_clean/epoch14.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.4299\n",
            "Epoch 15/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.3807\n",
            "Epoch 15: saving model to ./checkpoints_alice_clean/epoch15.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.3802\n",
            "Epoch 16/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.3325\n",
            "Epoch 16: saving model to ./checkpoints_alice_clean/epoch16.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.3325\n",
            "Epoch 17/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.2937\n",
            "Epoch 17: saving model to ./checkpoints_alice_clean/epoch17.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.2938\n",
            "Epoch 18/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.2595\n",
            "Epoch 18: saving model to ./checkpoints_alice_clean/epoch18.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 1.2595\n",
            "Epoch 19/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.2212\n",
            "Epoch 19: saving model to ./checkpoints_alice_clean/epoch19.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.2221\n",
            "Epoch 20/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1970\n",
            "Epoch 20: saving model to ./checkpoints_alice_clean/epoch20.weights.h5\n",
            "\n",
            "================================================================================\n",
            "Sample after epoch 20:\n",
            "Alice and you, won t you ve fourd beating of that side of the soot for s meaning, that s the first bess parden way, if you wouldn t hear! I shan t look little tood at the juryo.\n",
            "What doe the this more to be walked an once if you great sutting on mind found the jury-boxtance, the Dormoubse you seemed that she would say.\n",
            "You may be the sat nabbit, but that _ca__ Mouse, Fif! There was gound the King said t\n",
            "================================================================================\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 1.1970\n",
            "Epoch 21/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.1697\n",
            "Epoch 21: saving model to ./checkpoints_alice_clean/epoch21.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 1.1697\n",
            "Epoch 22/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1310\n",
            "Epoch 22: saving model to ./checkpoints_alice_clean/epoch22.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 1.1312\n",
            "Epoch 23/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.1070\n",
            "Epoch 23: saving model to ./checkpoints_alice_clean/epoch23.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.1070\n",
            "Epoch 24/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0724\n",
            "Epoch 24: saving model to ./checkpoints_alice_clean/epoch24.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.0731\n",
            "Epoch 25/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0441\n",
            "Epoch 25: saving model to ./checkpoints_alice_clean/epoch25.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.0449\n",
            "Epoch 26/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0185\n",
            "Epoch 26: saving model to ./checkpoints_alice_clean/epoch26.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.0189\n",
            "Epoch 27/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.9928\n",
            "Epoch 27: saving model to ./checkpoints_alice_clean/epoch27.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.9928\n",
            "Epoch 28/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.9588\n",
            "Epoch 28: saving model to ./checkpoints_alice_clean/epoch28.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.9597\n",
            "Epoch 29/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.9370\n",
            "Epoch 29: saving model to ./checkpoints_alice_clean/epoch29.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.9375\n",
            "Epoch 30/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.9087\n",
            "Epoch 30: saving model to ./checkpoints_alice_clean/epoch30.weights.h5\n",
            "\n",
            "================================================================================\n",
            "Sample after epoch 30:\n",
            "Alice asing the rest of the jury arm, when she next the neight back to the twinkling of the ten of the tire.\n",
            "But I m for the miserable passion, said the Mock Turtle in a more of great hurry.\n",
            "Only then they ll see what I s_, though the Dormouse again the King as the dance? You are old, said the Hatter.\n",
            "I don t see you dear, I wish you ought to herself, as it last the Gryphon remeeded: where any left off \n",
            "================================================================================\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - loss: 0.9089\n",
            "Epoch 31/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8758\n",
            "Epoch 31: saving model to ./checkpoints_alice_clean/epoch31.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.8771\n",
            "Epoch 32/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8445\n",
            "Epoch 32: saving model to ./checkpoints_alice_clean/epoch32.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.8457\n",
            "Epoch 33/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.8184\n",
            "Epoch 33: saving model to ./checkpoints_alice_clean/epoch33.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.8195\n",
            "Epoch 34/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7880\n",
            "Epoch 34: saving model to ./checkpoints_alice_clean/epoch34.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.7885\n",
            "Epoch 35/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7580\n",
            "Epoch 35: saving model to ./checkpoints_alice_clean/epoch35.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.7584\n",
            "Epoch 36/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7366\n",
            "Epoch 36: saving model to ./checkpoints_alice_clean/epoch36.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.7367\n",
            "Epoch 37/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.7032\n",
            "Epoch 37: saving model to ./checkpoints_alice_clean/epoch37.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.7036\n",
            "Epoch 38/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6730\n",
            "Epoch 38: saving model to ./checkpoints_alice_clean/epoch38.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.6742\n",
            "Epoch 39/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6479\n",
            "Epoch 39: saving model to ./checkpoints_alice_clean/epoch39.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.6482\n",
            "Epoch 40/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.6166\n",
            "Epoch 40: saving model to ./checkpoints_alice_clean/epoch40.weights.h5\n",
            "\n",
            "================================================================================\n",
            "Sample after epoch 40:\n",
            "Alice ase a minute or two, three inches did not answer without a growl while as the March Hare in the window, and one any mine The case with one finger! How funny more thought for her, poor Alices listled.\n",
            "Oh, you re turned on.\n",
            "Onct, I m once more thought to herself That s nothing had a minute or two with the other, saying to herself, for she should meal course, till the evening, if I hadn t armuld, rat\n",
            "================================================================================\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 0.6179\n",
            "Epoch 41/50\n",
            "\u001b[1m20/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.5906\n",
            "Epoch 41: saving model to ./checkpoints_alice_clean/epoch41.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.5910\n",
            "Epoch 42/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5636\n",
            "Epoch 42: saving model to ./checkpoints_alice_clean/epoch42.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.5637\n",
            "Epoch 43/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.5289\n",
            "Epoch 43: saving model to ./checkpoints_alice_clean/epoch43.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.5300\n",
            "Epoch 44/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.5028\n",
            "Epoch 44: saving model to ./checkpoints_alice_clean/epoch44.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.5038\n",
            "Epoch 45/50\n",
            "\u001b[1m20/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4792\n",
            "Epoch 45: saving model to ./checkpoints_alice_clean/epoch45.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.4798\n",
            "Epoch 46/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4534\n",
            "Epoch 46: saving model to ./checkpoints_alice_clean/epoch46.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.4539\n",
            "Epoch 47/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4279\n",
            "Epoch 47: saving model to ./checkpoints_alice_clean/epoch47.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.4289\n",
            "Epoch 48/50\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 0.4049\n",
            "Epoch 48: saving model to ./checkpoints_alice_clean/epoch48.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.4053\n",
            "Epoch 49/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3853\n",
            "Epoch 49: saving model to ./checkpoints_alice_clean/epoch49.weights.h5\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.3861\n",
            "Epoch 50/50\n",
            "\u001b[1m19/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.3612\n",
            "Epoch 50: saving model to ./checkpoints_alice_clean/epoch50.weights.h5\n",
            "\n",
            "================================================================================\n",
            "Sample after epoch 50:\n",
            "Alice abidea, with a crowd of little Alice went on, What was silent.\n",
            "The King said to Alice.\n",
            "What _is_ the very middle of the cake under she heard the Mock Turtle replied, and then are, pleapes that it was wroting so curions take mise about; being said in a watch of trouble, your Majesty, he began.\n",
            "Of course not, And were read be treacle? said the Queen.\n",
            "But you could remember them, and when she had not\n",
            "================================================================================\n",
            "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: 0.3625\n",
            "\n",
            "================================================================================\n",
            "Final sample after training:\n",
            "Alice ase words, Alice s understand it must many one; so her fis! Oh, my dear of their hands and feet! (for which she got to the dearess we learn! There was a large candle is blown out.\n",
            "Behind her in anything people only doest the door about them in books again.\n",
            "But at any rate I m afraid, so she went on growing, and, as a lastly more than never long rangen looking at once, who my far another side of _what?_ The flame of the queer-tabes goor egid, with a smilen a Caucus-race? said the Caterpillar angente, and she treed time to my right size what you were then stool noticed anxiously round the chimney! Oh! So Bill s got buriget to day to think about it, just now, said the Duchess.\n",
            "Everything s got a minute, while Alice twonclured from what the pleaserroom, and her eye felt a very difficult figure of the best place of high, and was a little rellieve your head much pleased again.\n",
            "The for off about for some advoisay op one way off, and that s very curious to see what the next witness.\n",
            "And when d\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}